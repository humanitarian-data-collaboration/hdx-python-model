{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Algorithm\n",
    "\n",
    "This notebook outlines the mechanism for tag-prediction. The below cell contains all the parameters for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Toggle Model Parameters\n",
    "\n",
    "create_dataset = False #boolean to determine whether to download datasets from HDX Database vs. pre-loaded excel file\n",
    "SAMPLE_NUMBER_OF_DATASETS = 150 #number of training datasets to download\n",
    "use_randomized_sample_for_BOW = True #boolean to determine whether to take a random sample from the data to expedite \n",
    "#feature vectorization\n",
    "use_randomized_sample_for_training_data = False #boolean to determine whether randomized sampling is used to choose training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import itertools \n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from fasttext import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# fetching pre-trained word vectors\n",
    "import requests, zipfile, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No logging configuration parameter. Using default.\n",
      "Loading logging configuration from: c:\\users\\cmillsop\\source\\hdx-python-model\\.venv\\lib\\site-packages\\hdx\\utilities\\logging_configuration.yml\n",
      "INFO - 2019-07-30 13:20:14 - hdx.hdx_configuration - No HDX base configuration parameter. Using default base configuration file: c:\\users\\cmillsop\\source\\hdx-python-model\\.venv\\lib\\site-packages\\hdx\\hdx_base_configuration.yml.\n",
      "INFO - 2019-07-30 13:20:14 - hdx.hdx_configuration - Loading HDX base configuration from: c:\\users\\cmillsop\\source\\hdx-python-model\\.venv\\lib\\site-packages\\hdx\\hdx_base_configuration.yml\n",
      "INFO - 2019-07-30 13:20:14 - hdx.hdx_configuration - No HDX configuration parameter and no configuration file at default path: C:\\Users\\cmillsop\\.hdx_configuration.yml.\n",
      "INFO - 2019-07-30 13:20:14 - hdx.hdx_configuration - Read only access to HDX: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://data.humdata.org/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting up account connection to HDX Database\n",
    "setup_logging()\n",
    "Configuration.create(hdx_site='prod', user_agent='A_Quick_Example', hdx_read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified that wiki.en.bin exists\n"
     ]
    }
   ],
   "source": [
    "# Set up environment and word vectors\n",
    "\n",
    "# datasets scraped from HDX will be stored here temporarily\n",
    "try:\n",
    "    os.makedirs('datasets')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if not os.path.exists('wiki.en.bin'):\n",
    "    print('downloading word vectors')\n",
    "    url_path = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip'\n",
    "    request = None\n",
    "    try:\n",
    "        request = requests.get(url_path)\n",
    "    except:\n",
    "        print('The download failed, try downloading \"English: bin+text\" manually from: https://fasttext.cc/docs/en/pretrained-vectors.html')\n",
    "    print('extracting vectors')\n",
    "    file_name = 'wiki.en.bin'\n",
    "    archive = zipfile.ZipFile(io.BytesIO(request.content))\n",
    "    archive.extract(file_name)\n",
    "else:\n",
    "    print(\"verified that wiki.en.bin exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HELPER FUNCTIONS\n",
    "\n",
    "#Check if the dataset has at least 1 resource of the required file type(s).\n",
    "\n",
    "def check_type(dataset, file_types=[]):\n",
    "    temp_dataset = Dataset.read_from_hdx(dataset)\n",
    "    temp_dataset.separate_resources()\n",
    "    if (len(temp_dataset.resources) > 0):\n",
    "        if (len(file_types) > 0):\n",
    "            if (not set(temp_dataset.get_filetypes()).isdisjoint(file_types)): \n",
    "                    return True\n",
    "        else :\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#check if organization from HXL\n",
    "def check_organization(dataset):\n",
    "    if dataset.get_organization()['title'] != 'Humanitarian Exchange Language(HXL)':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cmillsop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "#Helper functions to preprocess data\n",
    "\n",
    "def lower_cols(lst):\n",
    "    #convert data to lowercases\n",
    "    return [word.lower() for word in lst]\n",
    "\n",
    "def remove_chars(lst):\n",
    "    #remove punctuation characters such as \",\", \"(\", \")\", \"\"\", \":\", \"/\",\".\" and \"_\".\n",
    "    #NOTE: PRESERVES WHITE SPACE.\n",
    "    cleaned = [re.sub('\\s+', ' ', mystring).strip() for mystring in lst]\n",
    "    cleaned = [re.sub(r'[[^A-Za-z0-9\\s]+]', ' ', mystr) for mystr in cleaned]\n",
    "    cleaned = [mystr.replace('_', ' ') for mystr in cleaned]\n",
    "    return cleaned\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(data_lst):\n",
    "    #remove stopwords from the data including 'the', 'and' etc. \n",
    "    wordsFiltered = []\n",
    "    for w in data_lst:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered\n",
    "\n",
    "def separate_words(lst):\n",
    "    #separate words according to capitalization. Ex: projectNumber --> project Number\n",
    "    wordsFiltered = []\n",
    "    for word in lst:\n",
    "        temp = re.sub( r\"([A-Z])\", r\" \\1\", word).split()\n",
    "        if not temp or (len(temp) == len(word)):\n",
    "            wordsFiltered.append(word)\n",
    "        else:     \n",
    "            string = ' '.join(temp)\n",
    "            wordsFiltered.append(string)\n",
    "    return wordsFiltered\n",
    "\n",
    "#Function to aggregate above preprocessing functions\n",
    "def clean_cols(data):\n",
    "    data = separate_words(data)\n",
    "    data = lower_cols(data)\n",
    "    data = remove_chars(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download one dataset with required type(s), read it into Dataframe, \n",
    "# add all Headers, Tags, Attributes, Data, Relative Column Position, Dataset_name, and Organizations to our DataFrame,\n",
    "# temporarily stores this data in the datasets folder,\n",
    "# and subsequently deletes the dataset\n",
    "\n",
    "def process_dataset(dataset, file_type, dataframe, download_path, index, row_limit = 10):\n",
    "    global count\n",
    "    organization = \"\"\n",
    "# Download one dataset and read it into a DataFrame \n",
    "    if (file_type == None):\n",
    "        url, path = dataset.resources[0].download(download_path)\n",
    "        pandas_dataset = pd.read_csv(path)\n",
    "    else:\n",
    "        if (file_type not in dataset.get_filetypes()):\n",
    "            return 'Error: Required file type not in dataset OR dataset does not contain any resources.'\n",
    "        try:\n",
    "            url, path = dataset.resources[dataset.get_filetypes().index(file_type)].download(download_path)\n",
    "            organization = dataset.get_organization()['title']\n",
    "            print('Resource URL %s downloaded to %s' % (url, path))\n",
    "            pandas_dataset = pd.read_csv(path, encoding='latin-1')\n",
    "            pandas_dataset = pandas_dataset.head(row_limit)\n",
    "        except:\n",
    "            return 'Unknown error.'\n",
    "    \n",
    "    # Add headers, tags and data to our DataFrame if current dataset not empty\n",
    "        if (not pandas_dataset.empty):\n",
    "            dataset_df = pandas_dataset\n",
    "            headers = list(dataset_df.columns.values)\n",
    "            headers = clean_cols(headers)\n",
    "            tags = list(dataset_df.iloc[0,:])\n",
    "            for i in range(len(headers)):\n",
    "                try:\n",
    "                    splitted = re.split('[(^\\s+)+#]', tags[i])\n",
    "                    splitted = list(filter(None, splitted))\n",
    "                    hashtag = splitted[0]\n",
    "                    attributes = splitted[1:]\n",
    "                    dic = {'Header': headers[i], 'Tag': hashtag, 'Attributes': attributes, \n",
    "                           'Data': list(dataset_df.iloc[1:, i]), \n",
    "                           'Relative Column Position': (i+1) / len(dataset_df.columns), \n",
    "                           'Dataset_name': os.path.basename(path), \n",
    "                           'Organization': organization,\n",
    "                           'Index': index}\n",
    "                    dataframe.loc[len(dataframe)] = dic\n",
    "                except:\n",
    "                    print(\"Error: different number of headers and tags\")\n",
    "            count += 1\n",
    "        os.remove(path)\n",
    "        print(\"File Removed!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3690"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for all datasets with HXL tags\n",
    "datasets_HXL = Dataset.search_in_hdx('HXL')\n",
    "len(datasets_HXL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for all Headers, Tags, Attributes, Data, Relative Column Position, Dataset_name, and Organizations\n",
    "\n",
    "col_names = ['Header', 'Tag', 'Attributes','Data','Relative Column Position','Dataset_name', 'Organization','Index']\n",
    "headers_and_tags= pd.DataFrame(columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean dataframe for model input\n",
    "headers_and_tags['Header'] = clean_cols(headers_and_tags['Header'])\n",
    "headers_and_tags['Data'] = remove_stop_words(headers_and_tags['Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading in n tagged datasets either from HDX Database or pre-loaded excel file\n",
    "count = 0\n",
    "if (create_dataset):\n",
    "    for i in range(SAMPLE_NUMBER_OF_DATASETS):\n",
    "        rand_dataset = np.random.randint(0, len(datasets_HXL))\n",
    "        process_dataset_2(datasets_HXL[rand_dataset], 'CSV', headers_and_tags, './datasets', count)\n",
    "        print(i)\n",
    "        \n",
    "    headers_and_tags.to_excel(\"headerandtag.xlsx\")\n",
    "else:\n",
    "    headers_and_tags = pd.read_excel(\"headertag_fulldataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Header</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Data</th>\n",
       "      <th>Relative Column Position</th>\n",
       "      <th>Dataset_name</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://api.idmcdb.org/api/disaster data?ci=hd...</td>\n",
       "      <td>valid_tag</td>\n",
       "      <td>[]</td>\n",
       "      <td>['#access', '#activity', '#adm1', '#adm2', '#a...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hashtag one-liner</td>\n",
       "      <td>description</td>\n",
       "      <td>['short', 'en']</td>\n",
       "      <td>['Access ability/constraints', 'Programme, pro...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hashtag long description</td>\n",
       "      <td>description</td>\n",
       "      <td>['long', 'en']</td>\n",
       "      <td>['Accessiblity and constraints on access to a ...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>release status</td>\n",
       "      <td>status</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Released', 'Released', 'Released', 'Released...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>data type restriction</td>\n",
       "      <td>valid_datatype</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, 'number', ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>first release</td>\n",
       "      <td>meta</td>\n",
       "      <td>['release']</td>\n",
       "      <td>['1.1', '1.0', '1.0', '1.0', '1.0', '1.0', '1....</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>default taxonomy</td>\n",
       "      <td>valid_vocab</td>\n",
       "      <td>['default']</td>\n",
       "      <td>[nan, nan, '+v_pcode', '+v_pcode', '+v_pcode',...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>category</td>\n",
       "      <td>meta</td>\n",
       "      <td>['category']</td>\n",
       "      <td>['1.3. Responses and other operations', '1.3. ...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>sample hxl</td>\n",
       "      <td>meta</td>\n",
       "      <td>['example', 'hxl']</td>\n",
       "      <td>['#access +type', '#activity +project', '#adm1...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>sample description</td>\n",
       "      <td>meta</td>\n",
       "      <td>['example', 'description', 'en']</td>\n",
       "      <td>['type of access being described', 'an aid pro...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>vocabulary identifier</td>\n",
       "      <td>vocab</td>\n",
       "      <td>['att']</td>\n",
       "      <td>['+v_currency', '+v_glide', '+v_iso3', '+v_och...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>vocabulary name</td>\n",
       "      <td>vocab</td>\n",
       "      <td>['name', 'en']</td>\n",
       "      <td>['ISO 4217 Currency Codes', 'GLobal IDEntifier...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>description</td>\n",
       "      <td>description</td>\n",
       "      <td>['en']</td>\n",
       "      <td>['Standard list of financial currency codes us...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>organisation maintaining</td>\n",
       "      <td>org</td>\n",
       "      <td>['maint']</td>\n",
       "      <td>['International Organization for Standardizati...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>home page</td>\n",
       "      <td>vocab</td>\n",
       "      <td>['url', 'home']</td>\n",
       "      <td>['https://en.wikipedia.org/wiki/ISO_4217', 'ht...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>id</td>\n",
       "      <td>meta</td>\n",
       "      <td>['id']</td>\n",
       "      <td>['2376', '2377', '2379', '2380', '2381', '2382...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>preferred term</td>\n",
       "      <td>x_role</td>\n",
       "      <td>['preferred']</td>\n",
       "      <td>['Administrative Officer', 'Head of Agency or ...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>scope</td>\n",
       "      <td>x_definition</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Responsible for Office Administration and ad...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>french term</td>\n",
       "      <td>x_role</td>\n",
       "      <td>['preferred', 'fr']</td>\n",
       "      <td>['Assistant administratif', \"Directeur d'agenc...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>spanish term</td>\n",
       "      <td>x_role</td>\n",
       "      <td>['preferred', 'es']</td>\n",
       "      <td>['Oficial administrativo', 'Director de Agenci...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>date added</td>\n",
       "      <td>date</td>\n",
       "      <td>['created']</td>\n",
       "      <td>['02/24/2017', '02/24/2017', '02/24/2017', '02...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>date updated</td>\n",
       "      <td>date</td>\n",
       "      <td>['updated']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>notes</td>\n",
       "      <td>meta</td>\n",
       "      <td>['comment']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>admin1name</td>\n",
       "      <td>Abia</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Adamawa', 'Akwa Ibom', 'Anambra', 'Bauchi', ...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Nga_pop_adm1_2016.csv.CSV</td>\n",
       "      <td>OCHA Nigeria</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>admin1pcode</td>\n",
       "      <td>NG001</td>\n",
       "      <td>[]</td>\n",
       "      <td>['NG002', 'NG003', 'NG004', 'NG005', 'NG006', ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Nga_pop_adm1_2016.csv.CSV</td>\n",
       "      <td>OCHA Nigeria</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Guinea', 'Guinea', 'Guinea', 'Guinea', 'Guin...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>status</td>\n",
       "      <td>status</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nan, 'Open', nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>verified</td>\n",
       "      <td>x_verified</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Y', 'Y', 'Y', 'N', 'Y', 'N', 'Y', 'Y', 'Y']</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>date</td>\n",
       "      <td>report_date</td>\n",
       "      <td>[]</td>\n",
       "      <td>['01/01/2002', '09/10/2014', '01/01/2002', '01...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>center id</td>\n",
       "      <td>loc_id</td>\n",
       "      <td>[]</td>\n",
       "      <td>['HM0096', 'MOH035', 'HM0097', 'HM0933', 'HM00...</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>5901</td>\n",
       "      <td>percentfunded</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'pct']</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fts_requirements_funding_twn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>5902</td>\n",
       "      <td>countrycode</td>\n",
       "      <td>country</td>\n",
       "      <td>['code']</td>\n",
       "      <td>['STP', 'STP', 'STP', 'STP', 'STP']</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>5903</td>\n",
       "      <td>id</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'fts_internal']</td>\n",
       "      <td>[nan, nan, nan, '202', nan]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>5904</td>\n",
       "      <td>name</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'name']</td>\n",
       "      <td>['Not specified', 'Not specified', 'Not specif...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>5905</td>\n",
       "      <td>code</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'external']</td>\n",
       "      <td>[nan, nan, nan, 'FXWAF0506', nan]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>5906</td>\n",
       "      <td>startdate</td>\n",
       "      <td>date</td>\n",
       "      <td>['start']</td>\n",
       "      <td>[nan, nan, nan, '2005-11-01', nan]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>5907</td>\n",
       "      <td>enddate</td>\n",
       "      <td>date</td>\n",
       "      <td>['end']</td>\n",
       "      <td>[nan, nan, nan, '2006-05-31', nan]</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>5908</td>\n",
       "      <td>year</td>\n",
       "      <td>date</td>\n",
       "      <td>['year']</td>\n",
       "      <td>['2008', '2007', '2006', '2005', '2005']</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>5909</td>\n",
       "      <td>requirements</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'required', 'usd']</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>5910</td>\n",
       "      <td>funding</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'total', 'usd']</td>\n",
       "      <td>['591716', '1101695', '72044', '321004', '57136']</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>5911</td>\n",
       "      <td>percentfunded</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'pct']</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>5912</td>\n",
       "      <td>countrycode</td>\n",
       "      <td>country</td>\n",
       "      <td>['code']</td>\n",
       "      <td>['CHE', 'CHE', 'CHE', 'CHE', 'CHE', 'CHE', 'CH...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>5913</td>\n",
       "      <td>id</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'fts_internal']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>5914</td>\n",
       "      <td>name</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'name']</td>\n",
       "      <td>['Not specified', 'Not specified', 'Not specif...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>5915</td>\n",
       "      <td>code</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'external']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>5916</td>\n",
       "      <td>startdate</td>\n",
       "      <td>date</td>\n",
       "      <td>['start']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>5917</td>\n",
       "      <td>enddate</td>\n",
       "      <td>date</td>\n",
       "      <td>['end']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>5918</td>\n",
       "      <td>year</td>\n",
       "      <td>date</td>\n",
       "      <td>['year']</td>\n",
       "      <td>['2021', '2020', '2019', '2018', '2017', '2016...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>5919</td>\n",
       "      <td>requirements</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'required', 'usd']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>5920</td>\n",
       "      <td>funding</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'total', 'usd']</td>\n",
       "      <td>['406504', '2117307', '2939092', '2642377', '7...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>5921</td>\n",
       "      <td>percentfunded</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'pct']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>5942</td>\n",
       "      <td>countrycode</td>\n",
       "      <td>country</td>\n",
       "      <td>['code']</td>\n",
       "      <td>['SDN', 'SDN', 'SDN', 'SDN', 'SDN', 'SDN', 'SD...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>5943</td>\n",
       "      <td>id</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'fts_internal']</td>\n",
       "      <td>['635', '635', '635', '635', '635', '635', '63...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>5944</td>\n",
       "      <td>name</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'name']</td>\n",
       "      <td>['Sudan 2018', 'Sudan 2018', 'Sudan 2018', 'Su...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>5945</td>\n",
       "      <td>code</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'external']</td>\n",
       "      <td>['HSDN18', 'HSDN18', 'HSDN18', 'HSDN18', 'HSDN...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>5946</td>\n",
       "      <td>startdate</td>\n",
       "      <td>date</td>\n",
       "      <td>['start']</td>\n",
       "      <td>['2018-01-01', '2018-01-01', '2018-01-01', '20...</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>5947</td>\n",
       "      <td>enddate</td>\n",
       "      <td>date</td>\n",
       "      <td>['end']</td>\n",
       "      <td>['2018-12-24', '2018-12-24', '2018-12-24', '20...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5948</td>\n",
       "      <td>year</td>\n",
       "      <td>date</td>\n",
       "      <td>['year']</td>\n",
       "      <td>['2018', '2018', '2018', '2018', '2018', '2018...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>5949</td>\n",
       "      <td>clustercode</td>\n",
       "      <td>sector</td>\n",
       "      <td>['cluster', 'code']</td>\n",
       "      <td>['3980', '3981', '3982', '3983', '3985', '3986...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>5950</td>\n",
       "      <td>cluster</td>\n",
       "      <td>sector</td>\n",
       "      <td>['cluster', 'name']</td>\n",
       "      <td>['Common Services', 'Education', 'Emergency Sh...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                             Header  \\\n",
       "0             0  https://api.idmcdb.org/api/disaster data?ci=hd...   \n",
       "1             1                                  hashtag one-liner   \n",
       "2             2                           hashtag long description   \n",
       "3             3                                     release status   \n",
       "4             4                              data type restriction   \n",
       "..          ...                                                ...   \n",
       "195        5946                                          startdate   \n",
       "196        5947                                            enddate   \n",
       "197        5948                                               year   \n",
       "198        5949                                        clustercode   \n",
       "199        5950                                            cluster   \n",
       "\n",
       "                Tag           Attributes  \\\n",
       "0         valid_tag                   []   \n",
       "1       description      ['short', 'en']   \n",
       "2       description       ['long', 'en']   \n",
       "3            status                   []   \n",
       "4    valid_datatype                   []   \n",
       "..              ...                  ...   \n",
       "195            date            ['start']   \n",
       "196            date              ['end']   \n",
       "197            date             ['year']   \n",
       "198          sector  ['cluster', 'code']   \n",
       "199          sector  ['cluster', 'name']   \n",
       "\n",
       "                                                  Data  \\\n",
       "0    ['#access', '#activity', '#adm1', '#adm2', '#a...   \n",
       "1    ['Access ability/constraints', 'Programme, pro...   \n",
       "2    ['Accessiblity and constraints on access to a ...   \n",
       "3    ['Released', 'Released', 'Released', 'Released...   \n",
       "4    [nan, nan, nan, nan, nan, nan, nan, 'number', ...   \n",
       "..                                                 ...   \n",
       "195  ['2018-01-01', '2018-01-01', '2018-01-01', '20...   \n",
       "196  ['2018-12-24', '2018-12-24', '2018-12-24', '20...   \n",
       "197  ['2018', '2018', '2018', '2018', '2018', '2018...   \n",
       "198  ['3980', '3981', '3982', '3983', '3985', '3986...   \n",
       "199  ['Common Services', 'Education', 'Emergency Sh...   \n",
       "\n",
       "     Relative Column Position                                  Dataset_name  \\\n",
       "0                    0.100000               hxl-core-hashtag-schema.csv.CSV   \n",
       "1                    0.200000               hxl-core-hashtag-schema.csv.CSV   \n",
       "2                    0.300000               hxl-core-hashtag-schema.csv.CSV   \n",
       "3                    0.400000               hxl-core-hashtag-schema.csv.CSV   \n",
       "4                    0.500000               hxl-core-hashtag-schema.csv.CSV   \n",
       "..                        ...                                           ...   \n",
       "195                  0.416667  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "196                  0.500000  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "197                  0.583333  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "198                  0.666667  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "199                  0.750000  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "\n",
       "                             Organization  Index  \n",
       "0    Humanitarian Exchange Language (HXL)      0  \n",
       "1    Humanitarian Exchange Language (HXL)      0  \n",
       "2    Humanitarian Exchange Language (HXL)      0  \n",
       "3    Humanitarian Exchange Language (HXL)      0  \n",
       "4    Humanitarian Exchange Language (HXL)      0  \n",
       "..                                    ...    ...  \n",
       "195                              OCHA FTS    341  \n",
       "196                              OCHA FTS    341  \n",
       "197                              OCHA FTS    341  \n",
       "198                              OCHA FTS    341  \n",
       "199                              OCHA FTS    341  \n",
       "\n",
       "[200 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "headers_and_tags.head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers_and_tags.to_excel('processed_input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_randomly(dataset, threshold = 200):\n",
    "    #ensures that dataset isn't skewed towards particular tags by ensuring that each tag has at most a given number \n",
    "    #(default: 200) of rows defined by the threshold\n",
    "    new_dataset = dataset\n",
    "    tags_to_be_pruned = dataset['Tag'].value_counts()[dataset['Tag'].value_counts() > threshold].keys().tolist()\n",
    "    for tag in tags_to_be_pruned:\n",
    "        count = dataset['Tag'][dataset['Tag'] == tag].value_counts()\n",
    "        drop_count = count - threshold\n",
    "        drop_count = drop_count.tolist()[0]\n",
    "        new_dataset = new_dataset.drop(np.random.choice(new_dataset[new_dataset['Tag']==tag].index,size=drop_count,replace=False))\n",
    "    return new_dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#sample the data to avoid favoring frequently appearing words\n",
    "if (use_randomized_sample_for_training_data):\n",
    "    headers_and_tags = select_randomly(headers_and_tags)\n",
    "    headers_and_tags.to_excel('processed_input_random_selection.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implementing n-grams Model (not used in this model.)\n",
    "from nltk import ngrams\n",
    "\n",
    "def generate_n_grams(data_lst, n):\n",
    "    cleaned = remove_chars(list(data_lst))\n",
    "    cleaned = clean_cols(cleaned)\n",
    "    cleaned = remove_stop_words(cleaned) \n",
    "    return list(ngrams(cleaned, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a n-gram frequency table \n",
    "\n",
    "def count_stats_grams(two_d_arr):\n",
    "    lst = np.array([])\n",
    "    count = 0\n",
    "    singles_count = 0\n",
    "    multiples_count = 0\n",
    "    for arr in two_d_arr:\n",
    "        if arr not in lst:\n",
    "            count += 1\n",
    "            np.append(lst, arr)\n",
    "        if two_d_arr.count(arr) == 1:\n",
    "            singles_count += 1\n",
    "        if two_d_arr.count(arr) > 1:\n",
    "            multiples_count += 1\n",
    "    check = count - singles_count\n",
    "    assert(check == multiples_count)\n",
    "    return count, singles_count, multiples_count\n",
    "\n",
    "def n_gram_freqs(dataframe, max_n = 4):\n",
    "    n_gram_cols = ['n-gram', 'data' ,'unique ngrams', 'multiples', 'singles']\n",
    "    n_gram_freqs = pd.DataFrame(columns = n_gram_cols)\n",
    "    for i in range(max_n):\n",
    "        n = i+1\n",
    "        n_grams = generate_n_grams(dataframe['Header'], n)\n",
    "        unique_n_grams, singles, multiples = count_stats_grams(n_grams)\n",
    "        row = {'n-gram': n, \n",
    "              'data': n_grams,\n",
    "              'unique ngrams': unique_n_grams,\n",
    "              'multiples': multiples,\n",
    "              'singles': singles}\n",
    "        n_gram_freqs.loc[len(n_gram_freqs)] = row\n",
    "    return pd.DataFrame(n_gram_freqs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes a data row and cleans it for model input\n",
    "def word_extract(row):\n",
    "    ignore = ['nan']\n",
    "    no_white = [i.lstrip() for i in row if i not in ignore and not isinstance(i, float)]\n",
    "    cleaned_text = [w.lower() for w in no_white if w not in ignore]\n",
    "    return cleaned_text\n",
    "\n",
    "long_string = []\n",
    "for i in headers_and_tags['Data']:\n",
    "    result_by_tag = word_extract(i)\n",
    "    holder_list = ''.join(result_by_tag)\n",
    "    long_string.append(holder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loading in the pre-determined word vectors\n",
    "fasttext_model = 'wiki.en.bin'\n",
    "fmodel = load_model(fasttext_model)\n",
    "print(\"Pre-trained model loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper function to extract the top n most likely tags for a given column (3 top tags selected in default.)\n",
    "#returns a dictionary where key = header and values = dictionary of num_of_top_tags tags with their respective probabilities\n",
    "\n",
    "def top_tags(clf, X_test, series, num_of_top_tags = 3):\n",
    "    if (not isinstance(X_test, np.ndarray)):\n",
    "        X_test = X_test.values.tolist()\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    values = []\n",
    "    for i in range(len(X_test)):\n",
    "        max_args = probs[i].argsort()[-num_of_top_tags:][::-1]\n",
    "        top_suggested_tags = clf.classes_[max_args]\n",
    "        dictionary = {}\n",
    "        sorted_probs = np.take(probs[i], max_args)\n",
    "        key = series.iloc[i]\n",
    "        for j in range(len(top_suggested_tags)):\n",
    "            dictionary[top_suggested_tags[j]]=sorted_probs[j]\n",
    "        values.append((key, dictionary))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#helper functions to determine the confidence level of top predicted tag. Any tags with a confidence level < 0.5\n",
    "#will be discarded and replaced by a blank tag instead. The function returns an array of booleans determining whether \n",
    "#a given tag will be blank or not. \n",
    "\n",
    "def tag_predicted(clf, X_test, series, threshold):\n",
    "    #True if tag should be left blank\n",
    "    if (not isinstance(X_test, np.ndarray)):\n",
    "        X_test = X_test.values.tolist()\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    values = []\n",
    "    for i in range(len(X_test)):\n",
    "        max_arg = probs[i].argsort()[-1]\n",
    "        top_suggested_tag = clf.classes_[max_arg]\n",
    "        prob = np.take(probs[i], max_arg)\n",
    "        if (prob > threshold):\n",
    "            values.append(False)\n",
    "        else:\n",
    "            values.append(True)\n",
    "    return values\n",
    "\n",
    "#helper function to fill in the blanks for tags that have a confidence level less than the threshold\n",
    "def fill_blank_tags(predicted_tags, clf, X_test, series, threshold = 0.5):\n",
    "    boolean_array = tag_predicted(clf, X_test, series, threshold)\n",
    "    for i in range(len(predicted_tags)):\n",
    "        if (boolean_array[i] == True):\n",
    "            predicted_tags[i] = ''\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#helper function to extract n random data points from the data column and vectorizes them into columns\n",
    "def separate_words(series): \n",
    "    #each series is a long string that contains all the data\n",
    "    if (not isinstance(series, str)):\n",
    "        series = str(series)\n",
    "    lst = re.split(r\"[^a-zA-Z0-9_.]\", series)\n",
    "    lst = list(filter(None, lst))\n",
    "    return lst\n",
    "    \n",
    "def vectorize_n_datapoints(number_of_datapoints_to_vectorize = 7):\n",
    "    df['Data_separated'] = df['Data'].apply(separate_words)\n",
    "    if (number_of_datapoints_to_vectorize > len(df['Data_separated'][0])):\n",
    "        number_of_datapoints_to_vectorize = len(df['Data_separated'][0])\n",
    "    for i in range(number_of_datapoints_to_vectorize):\n",
    "        df['datapoint' + str(i)] = df['Data_separated'].str[i]\n",
    "        \n",
    "def embedded_datapoints(number_of_data_point_to_vectorize = 7):\n",
    "    vectorize_n_datapoints()\n",
    "    for i in range(number_of_data_point_to_vectorize):\n",
    "        df['embedded_datapoint' + str(i)] = df['datapoint' + str(i)].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "        \n",
    "number_of_data_point_to_vectorize = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#post-processing function that 1) checks tags with low confidence against mappings 2) fills in a blank prediction for \n",
    "#tags with a confidence level lower than threshold and had no obvious mappings associated with the predicted tag. \n",
    "#The function returns 1) the count of corrected tags 2) predicted and predicted tags\n",
    "\n",
    "def check_mapping(header, predicted_tag):\n",
    "    MAPPINGS = {\n",
    "    \"#geo\" : ['lon', 'lat', 'latitude', 'longitude'], #words that would likely appear for #geo tag\n",
    "    \"#admin\" : ['county'], #words that would likely appear for #admin tag\n",
    "    \"#country\" :  ['country'], #words that would likely appear for #country tag\n",
    "    \"#date\" : ['year', 'date'], #words that would likely appear for #date tag\n",
    "    \"#funding\": ['funding', 'funded'], #words that would likely appear for #funding tag\n",
    "    \"#value\": ['percentfunded'], #words that would likely appear for #value tag\n",
    "    \"#org\":['organization', 'funder ref', 'org'], #words that would likely appear for #org tag\n",
    "    \"#status\":['status'], #words that would likely appear for #status tag\n",
    "    \"#sector\":['sector'], #words that would likely appear for #sector tag\n",
    "    \"#adm1\":['adm1', 'admin1'], #words that would likely appear for #adm1 tag\n",
    "    \"#adm2\":['adm2', 'admin2'], #words that would likely appear for #adm2 tag\n",
    "    \"#adm3\":['adm3', 'admin3'], #words that would likely appear for #adm3 tag\n",
    "    \"#adm4\":['adm4', 'admin4']  #words that would likely appear for #adm4 tag        \n",
    "    }\n",
    "    change_tag = False\n",
    "    header_words = header.split()\n",
    "    for key, val in MAPPINGS.items():\n",
    "        for word in header_words:\n",
    "            #check if the header contains any of the words in the mappings (substrings are not included)\n",
    "            if (word in val):\n",
    "                if (predicted_tag != key):\n",
    "                    predicted_tag = key\n",
    "                    change_tag = True\n",
    "    return change_tag, predicted_tag\n",
    "    \n",
    "\n",
    "def post_processing(headers, predicted_tags, clf, X_test, mapping_threshold = 0.85, blank_threshold = 0.2):\n",
    "    if (not isinstance(X_test, np.ndarray)):\n",
    "        X_test = X_test.values.tolist()\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    values = []\n",
    "    corrected_count = 0\n",
    "    blank_count = 0\n",
    "    for i in range(len(X_test)):\n",
    "        max_arg = probs[i].argsort()[-1]\n",
    "        top_suggested_tag = clf.classes_[max_arg]\n",
    "        prob = np.take(probs[i], max_arg)\n",
    "        header = headers.tolist()[i]\n",
    "        predicted_tag = predicted_tags[i]\n",
    "        if (prob < mapping_threshold):\n",
    "            inc, predicted_tag = check_mapping(header, predicted_tag)\n",
    "            if (inc):\n",
    "                corrected_count += 1\n",
    "            else:\n",
    "                if (prob < blank_threshold): \n",
    "                    predicted_tag = ''\n",
    "                    blank_count += 1\n",
    "        values.append(predicted_tag)\n",
    "    return corrected_count, blank_count, values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper function to add hashtags to the predicted tags\n",
    "def add_hashtags(predicted_tags):\n",
    "    if (isinstance(predicted_tags, np.ndarray)):\n",
    "        return [\"#\"+word for word in predicted_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings extracted!\n",
      "\n",
      "Features created!\n"
     ]
    }
   ],
   "source": [
    "#CREATING THE MODEL and TESTING ACCURACY OF MLP Classifer on all features. This is the model used in the API.\n",
    "\n",
    "#using all three embedded features to predict tags\n",
    "#cleaning the dataset by flattening the datastructure\n",
    "\n",
    "df = headers_and_tags\n",
    "\n",
    "df['Header_embedding'] = df['Header'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "df['Organization_embedded'] = df['Organization'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "embedded_datapoints()\n",
    "print(\"Word embeddings extracted!\\n\")\n",
    "\n",
    "df['data_combined'] = df.loc[:, 'embedded_datapoint0': 'embedded_datapoint' \n",
    "                                                           + str(number_of_data_point_to_vectorize-1)].values.tolist()\n",
    "df['data_combined'] = df['data_combined'].apply(lambda x: [val for item in x for val in item])\n",
    "\n",
    "df_target = df\n",
    "cols = ['Header_embedding', 'Organization_embedded', 'data_combined']\n",
    "df_target['features_combined'] = df_target[cols].values.tolist()\n",
    "df_target['features_combined'] = df_target['features_combined'].apply(lambda x: [val for item in x for val in item])\n",
    "print(\"Features created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags corrected: 13\n",
      "Number of blank tags: 31\n",
      "Classification accuracy on test set: 0.9460539460539461\n"
     ]
    }
   ],
   "source": [
    "#Outputs: predicted_tags = 1D array including the most likely tags for each column\n",
    "#         top_3_predicted_tags = a dictionary of 3 most likely tags with their respective probabilities for a given column\n",
    "#         pickle file = stores the classifier to be inputted into the API\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_target['features_combined'], \n",
    "                                                    df['Tag'][0:len(df_target['features_combined'])], \n",
    "                                                    test_size=0.33, random_state=0)\n",
    "\n",
    "_1, headers, _2, _3 = train_test_split(df_target['Header'][0:len(df_target['features_combined'])], \n",
    "                                       df['Tag'][0:len(df_target['features_combined'])], test_size = 0.33, random_state = 0)\n",
    "\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "clf.fit(list(X_train), y_train)\n",
    "predicted_tags = clf.predict(list(X_test))\n",
    "predicted_tags = add_hashtags(predicted_tags)\n",
    "\n",
    "corrected_count, blank_count, predicted_tags = post_processing(headers, predicted_tags, clf, X_test)\n",
    "print(\"Number of tags corrected: \" + str(corrected_count))\n",
    "print(\"Number of blank tags: \" + str(blank_count))\n",
    "\n",
    "top_3_predicted_tags = top_tags(clf, X_test, df['Header'])\n",
    "#predicted_tags = fill_blank_tags(predicted_tags, clf, X_test, df['Header'])\n",
    "test_score = clf.score(list(X_test), y_test)\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)\n",
    "pickle.dump(clf,open(\"model.pkl\",\"wb\"))\n",
    "pickle.dump(predicted_tags, open(\"predicted_tags.pkl\",\"wb\"))\n",
    "pickle.dump(top_3_predicted_tags, open(\"top_three_predicted_tags.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing The Accuracy for Other Models using all four features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "names = [\"Nearest Neighbors\",\n",
    "         \"Random Forest\", \"Neural Net\",\n",
    "         \"Naive Bayes\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    #SVC(kernel=\"linear\", C=0.025),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),#use graph search? toggle max_features\n",
    "    MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam'),\n",
    "    GaussianNB()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "i = 1\n",
    "for name, clf in zip(names, classifiers):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_target['features_combined'], \n",
    "                                                    df['Tag'][0:len(df_target['features_combined'])], \n",
    "                                                    test_size=0.33, random_state=0) \n",
    "        clf.fit(list(X_train), y_train)\n",
    "        score = clf.score(list(X_test), y_test)\n",
    "        print(name)\n",
    "        print(score)\n",
    "        \n",
    "\n",
    "#Accuracy of MLP: 95.10%\n",
    "\n",
    "#Models Considered\n",
    "#1) Gaussian Naive Bayes       \n",
    "    #Accuracy: 88.03%\n",
    "#2) RandomForest       \n",
    "    #Accuracy: 98.36%"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "hdx-model",
   "language": "python",
   "name": "hdx-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
