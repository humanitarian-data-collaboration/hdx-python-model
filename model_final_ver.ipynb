{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Toggle Model Parameters\n",
    "\n",
    "create_dataset = False #boolean to determine whether to download datasets from HDX Database vs. pre-loaded excel file\n",
    "SAMPLE_NUMBER_OF_DATASETS = 150 #number of training datasets to download\n",
    "use_randomized_sample_for_BOW = True #boolean to determine whether to take a random sample from the data to expedite \n",
    "#feature vectorization\n",
    "use_randomized_sample_for_training_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import itertools \n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from fastText import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No logging configuration parameter. Using default.\n",
      "Loading logging configuration from: c:\\users\\cheneli\\anaconda3\\lib\\site-packages\\hdx\\utilities\\logging_configuration.yml\n",
      "INFO - 2019-07-19 11:35:18 - hdx.hdx_configuration - No HDX base configuration parameter. Using default base configuration file: c:\\users\\cheneli\\anaconda3\\lib\\site-packages\\hdx\\hdx_base_configuration.yml.\n",
      "INFO - 2019-07-19 11:35:18 - hdx.hdx_configuration - Loading HDX base configuration from: c:\\users\\cheneli\\anaconda3\\lib\\site-packages\\hdx\\hdx_base_configuration.yml\n",
      "INFO - 2019-07-19 11:35:18 - hdx.hdx_configuration - No HDX configuration parameter and no configuration file at default path: C:\\Users\\cheneli\\.hdx_configuration.yml.\n",
      "INFO - 2019-07-19 11:35:18 - hdx.hdx_configuration - Read only access to HDX: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://data.humdata.org/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting up account connection to HDX Database\n",
    "setup_logging()\n",
    "Configuration.create(hdx_site='prod', user_agent='A_Quick_Example', hdx_read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HELPER FUNCTIONS\n",
    "\n",
    "#Check if the dataset has at least 1 resource of the required file type(s).\n",
    "\n",
    "def check_type(dataset, file_types=[]):\n",
    "    temp_dataset = Dataset.read_from_hdx(dataset)\n",
    "    temp_dataset.separate_resources()\n",
    "    if (len(temp_dataset.resources) > 0):\n",
    "        if (len(file_types) > 0):\n",
    "            if (not set(temp_dataset.get_filetypes()).isdisjoint(file_types)): \n",
    "                    return True\n",
    "        else :\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#check if organization from HXL\n",
    "def check_organization(dataset):\n",
    "    if dataset.get_organization()['title'] != 'Humanitarian Exchange Language(HXL)':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cheneli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "#Helper functions to preprocess data\n",
    "\n",
    "def lower_cols(lst):\n",
    "    #convert data to lowercases\n",
    "    return [word.lower() for word in lst]\n",
    "\n",
    "def remove_chars(lst):\n",
    "    #remove punctuation characters such as \",\", \"(\", \")\", \"\"\", \":\", \"/\",\".\" and \"_\".\n",
    "    #NOTE: PRESERVES WHITE SPACE.\n",
    "    cleaned = [re.sub('\\s+', ' ', mystring).strip() for mystring in lst]\n",
    "    cleaned = [re.sub(r'[[^A-Za-z0-9\\s]+]', ' ', mystr) for mystr in cleaned]\n",
    "    cleaned = [mystr.replace('_', ' ') for mystr in cleaned]\n",
    "    return cleaned\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(data_lst):\n",
    "    #remove stopwords from the data including 'the', 'and' etc. \n",
    "    wordsFiltered = []\n",
    "    for w in data_lst:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered\n",
    "\n",
    "def separate_words(lst):\n",
    "    #separate words according to capitalization. Ex: projectNumber --> project Number\n",
    "    wordsFiltered = []\n",
    "    for word in lst:\n",
    "        temp = re.sub( r\"([A-Z])\", r\" \\1\", word).split()\n",
    "        if not temp or (len(temp) == len(word)):\n",
    "            wordsFiltered.append(word)\n",
    "        else:     \n",
    "            string = ' '.join(temp)\n",
    "            wordsFiltered.append(string)\n",
    "    return wordsFiltered\n",
    "\n",
    "#Function to aggregate above preprocessing functions\n",
    "def clean_cols(data):\n",
    "    data = separate_words(data)\n",
    "    data = lower_cols(data)\n",
    "    data = remove_chars(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download one dataset with required type(s), read it into Dataframe, \n",
    "# add all Headers, Tags, Attributes, Data, Relative Column Position, Dataset_name, and Organizations to our DataFrame,\n",
    "# temporarily stores this data in the datasets folder,\n",
    "# and subsequently deletes the dataset\n",
    "\n",
    "def process_dataset(dataset, file_type, dataframe, download_path, index, row_limit = 10):\n",
    "    global count\n",
    "    organization = \"\"\n",
    "# Download one dataset and read it into a DataFrame \n",
    "    if (file_type == None):\n",
    "        url, path = dataset.resources[0].download(download_path)\n",
    "        pandas_dataset = pd.read_csv(path)\n",
    "    else:\n",
    "        if (file_type not in dataset.get_filetypes()):\n",
    "            return 'Error: Required file type not in dataset OR dataset does not contain any resources.'\n",
    "        try:\n",
    "            url, path = dataset.resources[dataset.get_filetypes().index(file_type)].download(download_path)\n",
    "            organization = dataset.get_organization()['title']\n",
    "            print('Resource URL %s downloaded to %s' % (url, path))\n",
    "            pandas_dataset = pd.read_csv(path, encoding='latin-1')\n",
    "            pandas_dataset = pandas_dataset.head(row_limit)\n",
    "        except:\n",
    "            return 'Unknown error.'\n",
    "    \n",
    "    # Add headers, tags and data to our DataFrame if current dataset not empty\n",
    "        if (not pandas_dataset.empty):\n",
    "            dataset_df = pandas_dataset\n",
    "            headers = list(dataset_df.columns.values)\n",
    "            headers = clean_cols(headers)\n",
    "            tags = list(dataset_df.iloc[0,:])\n",
    "            for i in range(len(headers)):\n",
    "                try:\n",
    "                    splitted = re.split('[(^\\s+)+#]', tags[i])\n",
    "                    splitted = list(filter(None, splitted))\n",
    "                    hashtag = splitted[0]\n",
    "                    attributes = splitted[1:]\n",
    "                    dic = {'Header': headers[i], 'Tag': hashtag, 'Attributes': attributes, \n",
    "                           'Data': list(dataset_df.iloc[1:, i]), \n",
    "                           'Relative Column Position': (i+1) / len(dataset_df.columns), \n",
    "                           'Dataset_name': os.path.basename(path), \n",
    "                           'Organization': organization,\n",
    "                           'Index': index}\n",
    "                    dataframe.loc[len(dataframe)] = dic\n",
    "                except:\n",
    "                    print(\"Error: different number of headers and tags\")\n",
    "            count += 1\n",
    "        os.remove(path)\n",
    "        print(\"File Removed!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3684"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for all datasets with HXL tags\n",
    "datasets_HXL = Dataset.search_in_hdx('HXL')\n",
    "len(datasets_HXL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for all Headers, Tags, Attributes, Data, Relative Column Position, Dataset_name, and Organizations\n",
    "\n",
    "col_names = ['Header', 'Tag', 'Attributes','Data','Relative Column Position','Dataset_name', 'Organization','Index']\n",
    "headers_and_tags= pd.DataFrame(columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean dataframe for model input\n",
    "headers_and_tags['Header'] = clean_cols(headers_and_tags['Header'])\n",
    "headers_and_tags['Data'] = remove_stop_words(headers_and_tags['Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading in n tagged datasets either from HDX Database or pre-loaded excel file\n",
    "count = 0\n",
    "if (create_dataset):\n",
    "    for i in range(SAMPLE_NUMBER_OF_DATASETS):\n",
    "        rand_dataset = np.random.randint(0, len(datasets_HXL))\n",
    "        process_dataset_2(datasets_HXL[rand_dataset], 'CSV', headers_and_tags, './datasets', count)\n",
    "        print(i)\n",
    "        \n",
    "    headers_and_tags.to_excel(\"headerandtag.xlsx\")\n",
    "else:\n",
    "    headers_and_tags = pd.read_excel(\"headertag_fulldataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Data</th>\n",
       "      <th>Relative Column Position</th>\n",
       "      <th>Dataset_name</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.idmcdb.org/api/disaster data?ci=hd...</td>\n",
       "      <td>valid_tag</td>\n",
       "      <td>[]</td>\n",
       "      <td>['#access', '#activity', '#adm1', '#adm2', '#a...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hashtag one-liner</td>\n",
       "      <td>description</td>\n",
       "      <td>['short', 'en']</td>\n",
       "      <td>['Access ability/constraints', 'Programme, pro...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hashtag long description</td>\n",
       "      <td>description</td>\n",
       "      <td>['long', 'en']</td>\n",
       "      <td>['Accessiblity and constraints on access to a ...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>release status</td>\n",
       "      <td>status</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Released', 'Released', 'Released', 'Released...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data type restriction</td>\n",
       "      <td>valid_datatype</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, 'number', ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>first release</td>\n",
       "      <td>meta</td>\n",
       "      <td>['release']</td>\n",
       "      <td>['1.1', '1.0', '1.0', '1.0', '1.0', '1.0', '1....</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>default taxonomy</td>\n",
       "      <td>valid_vocab</td>\n",
       "      <td>['default']</td>\n",
       "      <td>[nan, nan, '+v_pcode', '+v_pcode', '+v_pcode',...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>category</td>\n",
       "      <td>meta</td>\n",
       "      <td>['category']</td>\n",
       "      <td>['1.3. Responses and other operations', '1.3. ...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sample hxl</td>\n",
       "      <td>meta</td>\n",
       "      <td>['example', 'hxl']</td>\n",
       "      <td>['#access +type', '#activity +project', '#adm1...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sample description</td>\n",
       "      <td>meta</td>\n",
       "      <td>['example', 'description', 'en']</td>\n",
       "      <td>['type of access being described', 'an aid pro...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>hxl-core-hashtag-schema.csv.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vocabulary identifier</td>\n",
       "      <td>vocab</td>\n",
       "      <td>['att']</td>\n",
       "      <td>['+v_currency', '+v_glide', '+v_iso3', '+v_och...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vocabulary name</td>\n",
       "      <td>vocab</td>\n",
       "      <td>['name', 'en']</td>\n",
       "      <td>['ISO 4217 Currency Codes', 'GLobal IDEntifier...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>description</td>\n",
       "      <td>description</td>\n",
       "      <td>['en']</td>\n",
       "      <td>['Standard list of financial currency codes us...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>organisation maintaining</td>\n",
       "      <td>org</td>\n",
       "      <td>['maint']</td>\n",
       "      <td>['International Organization for Standardizati...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>home page</td>\n",
       "      <td>vocab</td>\n",
       "      <td>['url', 'home']</td>\n",
       "      <td>['https://en.wikipedia.org/wiki/ISO_4217', 'ht...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>HXL master vocabulary list.CSV</td>\n",
       "      <td>Humanitarian Exchange Language (HXL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>id</td>\n",
       "      <td>meta</td>\n",
       "      <td>['id']</td>\n",
       "      <td>['2376', '2377', '2379', '2380', '2381', '2382...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>preferred term</td>\n",
       "      <td>x_role</td>\n",
       "      <td>['preferred']</td>\n",
       "      <td>['Administrative Officer', 'Head of Agency or ...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scope</td>\n",
       "      <td>x_definition</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Responsible for Office Administration and ad...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>french term</td>\n",
       "      <td>x_role</td>\n",
       "      <td>['preferred', 'fr']</td>\n",
       "      <td>['Assistant administratif', \"Directeur d'agenc...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spanish term</td>\n",
       "      <td>x_role</td>\n",
       "      <td>['preferred', 'es']</td>\n",
       "      <td>['Oficial administrativo', 'Director de Agenci...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>date added</td>\n",
       "      <td>date</td>\n",
       "      <td>['created']</td>\n",
       "      <td>['02/24/2017', '02/24/2017', '02/24/2017', '02...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>date updated</td>\n",
       "      <td>date</td>\n",
       "      <td>['updated']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>notes</td>\n",
       "      <td>meta</td>\n",
       "      <td>['comment']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Functional Roles (Beta) - CSV.CSV</td>\n",
       "      <td>OCHA Digital Services</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>admin1name</td>\n",
       "      <td>Abia</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Adamawa', 'Akwa Ibom', 'Anambra', 'Bauchi', ...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Nga_pop_adm1_2016.csv.CSV</td>\n",
       "      <td>OCHA Nigeria</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>admin1pcode</td>\n",
       "      <td>NG001</td>\n",
       "      <td>[]</td>\n",
       "      <td>['NG002', 'NG003', 'NG004', 'NG005', 'NG006', ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Nga_pop_adm1_2016.csv.CSV</td>\n",
       "      <td>OCHA Nigeria</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Guinea', 'Guinea', 'Guinea', 'Guinea', 'Guin...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>status</td>\n",
       "      <td>status</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nan, 'Open', nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>verified</td>\n",
       "      <td>x_verified</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Y', 'Y', 'Y', 'N', 'Y', 'N', 'Y', 'Y', 'Y']</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>date</td>\n",
       "      <td>report_date</td>\n",
       "      <td>[]</td>\n",
       "      <td>['01/01/2002', '09/10/2014', '01/01/2002', '01...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>center id</td>\n",
       "      <td>loc_id</td>\n",
       "      <td>[]</td>\n",
       "      <td>['HM0096', 'MOH035', 'HM0097', 'HM0933', 'HM00...</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>Guinea Health Facilities.CSV</td>\n",
       "      <td>Standby Task Force</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>percentfunded</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'pct']</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fts_requirements_funding_twn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>countrycode</td>\n",
       "      <td>country</td>\n",
       "      <td>['code']</td>\n",
       "      <td>['STP', 'STP', 'STP', 'STP', 'STP']</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>id</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'fts_internal']</td>\n",
       "      <td>[nan, nan, nan, '202', nan]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>name</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'name']</td>\n",
       "      <td>['Not specified', 'Not specified', 'Not specif...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>code</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'external']</td>\n",
       "      <td>[nan, nan, nan, 'FXWAF0506', nan]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>startdate</td>\n",
       "      <td>date</td>\n",
       "      <td>['start']</td>\n",
       "      <td>[nan, nan, nan, '2005-11-01', nan]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>enddate</td>\n",
       "      <td>date</td>\n",
       "      <td>['end']</td>\n",
       "      <td>[nan, nan, nan, '2006-05-31', nan]</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>year</td>\n",
       "      <td>date</td>\n",
       "      <td>['year']</td>\n",
       "      <td>['2008', '2007', '2006', '2005', '2005']</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>requirements</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'required', 'usd']</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>funding</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'total', 'usd']</td>\n",
       "      <td>['591716', '1101695', '72044', '321004', '57136']</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5911</th>\n",
       "      <td>percentfunded</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'pct']</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fts_requirements_funding_stp.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5912</th>\n",
       "      <td>countrycode</td>\n",
       "      <td>country</td>\n",
       "      <td>['code']</td>\n",
       "      <td>['CHE', 'CHE', 'CHE', 'CHE', 'CHE', 'CHE', 'CH...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>id</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'fts_internal']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5914</th>\n",
       "      <td>name</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'name']</td>\n",
       "      <td>['Not specified', 'Not specified', 'Not specif...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>code</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'external']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>startdate</td>\n",
       "      <td>date</td>\n",
       "      <td>['start']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>enddate</td>\n",
       "      <td>date</td>\n",
       "      <td>['end']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>year</td>\n",
       "      <td>date</td>\n",
       "      <td>['year']</td>\n",
       "      <td>['2021', '2020', '2019', '2018', '2017', '2016...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>requirements</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'required', 'usd']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>funding</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'total', 'usd']</td>\n",
       "      <td>['406504', '2117307', '2939092', '2642377', '7...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921</th>\n",
       "      <td>percentfunded</td>\n",
       "      <td>value</td>\n",
       "      <td>['funding', 'pct']</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fts_requirements_funding_che.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5942</th>\n",
       "      <td>countrycode</td>\n",
       "      <td>country</td>\n",
       "      <td>['code']</td>\n",
       "      <td>['SDN', 'SDN', 'SDN', 'SDN', 'SDN', 'SDN', 'SD...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5943</th>\n",
       "      <td>id</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'fts_internal']</td>\n",
       "      <td>['635', '635', '635', '635', '635', '635', '63...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5944</th>\n",
       "      <td>name</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'name']</td>\n",
       "      <td>['Sudan 2018', 'Sudan 2018', 'Sudan 2018', 'Su...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>code</td>\n",
       "      <td>activity</td>\n",
       "      <td>['appeal', 'id', 'external']</td>\n",
       "      <td>['HSDN18', 'HSDN18', 'HSDN18', 'HSDN18', 'HSDN...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>startdate</td>\n",
       "      <td>date</td>\n",
       "      <td>['start']</td>\n",
       "      <td>['2018-01-01', '2018-01-01', '2018-01-01', '20...</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>enddate</td>\n",
       "      <td>date</td>\n",
       "      <td>['end']</td>\n",
       "      <td>['2018-12-24', '2018-12-24', '2018-12-24', '20...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>year</td>\n",
       "      <td>date</td>\n",
       "      <td>['year']</td>\n",
       "      <td>['2018', '2018', '2018', '2018', '2018', '2018...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>clustercode</td>\n",
       "      <td>sector</td>\n",
       "      <td>['cluster', 'code']</td>\n",
       "      <td>['3980', '3981', '3982', '3983', '3985', '3986...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>cluster</td>\n",
       "      <td>sector</td>\n",
       "      <td>['cluster', 'name']</td>\n",
       "      <td>['Common Services', 'Education', 'Emergency Sh...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>fts_requirements_funding_cluster_sdn.csv.CSV</td>\n",
       "      <td>OCHA FTS</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Header             Tag  \\\n",
       "0     https://api.idmcdb.org/api/disaster data?ci=hd...       valid_tag   \n",
       "1                                     hashtag one-liner     description   \n",
       "2                              hashtag long description     description   \n",
       "3                                        release status          status   \n",
       "4                                 data type restriction  valid_datatype   \n",
       "5                                         first release            meta   \n",
       "6                                      default taxonomy     valid_vocab   \n",
       "7                                              category            meta   \n",
       "8                                            sample hxl            meta   \n",
       "9                                    sample description            meta   \n",
       "10                                vocabulary identifier           vocab   \n",
       "11                                      vocabulary name           vocab   \n",
       "12                                          description     description   \n",
       "13                             organisation maintaining             org   \n",
       "14                                            home page           vocab   \n",
       "15                                                   id            meta   \n",
       "16                                       preferred term          x_role   \n",
       "17                                                scope    x_definition   \n",
       "18                                          french term          x_role   \n",
       "19                                         spanish term          x_role   \n",
       "20                                           date added            date   \n",
       "21                                         date updated            date   \n",
       "22                                                notes            meta   \n",
       "23                                           admin1name            Abia   \n",
       "24                                          admin1pcode           NG001   \n",
       "25                                              country         country   \n",
       "26                                               status          status   \n",
       "27                                             verified      x_verified   \n",
       "28                                                 date     report_date   \n",
       "29                                            center id          loc_id   \n",
       "...                                                 ...             ...   \n",
       "5901                                      percentfunded           value   \n",
       "5902                                        countrycode         country   \n",
       "5903                                                 id        activity   \n",
       "5904                                               name        activity   \n",
       "5905                                               code        activity   \n",
       "5906                                          startdate            date   \n",
       "5907                                            enddate            date   \n",
       "5908                                               year            date   \n",
       "5909                                       requirements           value   \n",
       "5910                                            funding           value   \n",
       "5911                                      percentfunded           value   \n",
       "5912                                        countrycode         country   \n",
       "5913                                                 id        activity   \n",
       "5914                                               name        activity   \n",
       "5915                                               code        activity   \n",
       "5916                                          startdate            date   \n",
       "5917                                            enddate            date   \n",
       "5918                                               year            date   \n",
       "5919                                       requirements           value   \n",
       "5920                                            funding           value   \n",
       "5921                                      percentfunded           value   \n",
       "5942                                        countrycode         country   \n",
       "5943                                                 id        activity   \n",
       "5944                                               name        activity   \n",
       "5945                                               code        activity   \n",
       "5946                                          startdate            date   \n",
       "5947                                            enddate            date   \n",
       "5948                                               year            date   \n",
       "5949                                        clustercode          sector   \n",
       "5950                                            cluster          sector   \n",
       "\n",
       "                            Attributes  \\\n",
       "0                                   []   \n",
       "1                      ['short', 'en']   \n",
       "2                       ['long', 'en']   \n",
       "3                                   []   \n",
       "4                                   []   \n",
       "5                          ['release']   \n",
       "6                          ['default']   \n",
       "7                         ['category']   \n",
       "8                   ['example', 'hxl']   \n",
       "9     ['example', 'description', 'en']   \n",
       "10                             ['att']   \n",
       "11                      ['name', 'en']   \n",
       "12                              ['en']   \n",
       "13                           ['maint']   \n",
       "14                     ['url', 'home']   \n",
       "15                              ['id']   \n",
       "16                       ['preferred']   \n",
       "17                                  []   \n",
       "18                 ['preferred', 'fr']   \n",
       "19                 ['preferred', 'es']   \n",
       "20                         ['created']   \n",
       "21                         ['updated']   \n",
       "22                         ['comment']   \n",
       "23                                  []   \n",
       "24                                  []   \n",
       "25                                  []   \n",
       "26                                  []   \n",
       "27                                  []   \n",
       "28                                  []   \n",
       "29                                  []   \n",
       "...                                ...   \n",
       "5901                ['funding', 'pct']   \n",
       "5902                          ['code']   \n",
       "5903  ['appeal', 'id', 'fts_internal']   \n",
       "5904                ['appeal', 'name']   \n",
       "5905      ['appeal', 'id', 'external']   \n",
       "5906                         ['start']   \n",
       "5907                           ['end']   \n",
       "5908                          ['year']   \n",
       "5909    ['funding', 'required', 'usd']   \n",
       "5910       ['funding', 'total', 'usd']   \n",
       "5911                ['funding', 'pct']   \n",
       "5912                          ['code']   \n",
       "5913  ['appeal', 'id', 'fts_internal']   \n",
       "5914                ['appeal', 'name']   \n",
       "5915      ['appeal', 'id', 'external']   \n",
       "5916                         ['start']   \n",
       "5917                           ['end']   \n",
       "5918                          ['year']   \n",
       "5919    ['funding', 'required', 'usd']   \n",
       "5920       ['funding', 'total', 'usd']   \n",
       "5921                ['funding', 'pct']   \n",
       "5942                          ['code']   \n",
       "5943  ['appeal', 'id', 'fts_internal']   \n",
       "5944                ['appeal', 'name']   \n",
       "5945      ['appeal', 'id', 'external']   \n",
       "5946                         ['start']   \n",
       "5947                           ['end']   \n",
       "5948                          ['year']   \n",
       "5949               ['cluster', 'code']   \n",
       "5950               ['cluster', 'name']   \n",
       "\n",
       "                                                   Data  \\\n",
       "0     ['#access', '#activity', '#adm1', '#adm2', '#a...   \n",
       "1     ['Access ability/constraints', 'Programme, pro...   \n",
       "2     ['Accessiblity and constraints on access to a ...   \n",
       "3     ['Released', 'Released', 'Released', 'Released...   \n",
       "4     [nan, nan, nan, nan, nan, nan, nan, 'number', ...   \n",
       "5     ['1.1', '1.0', '1.0', '1.0', '1.0', '1.0', '1....   \n",
       "6     [nan, nan, '+v_pcode', '+v_pcode', '+v_pcode',...   \n",
       "7     ['1.3. Responses and other operations', '1.3. ...   \n",
       "8     ['#access +type', '#activity +project', '#adm1...   \n",
       "9     ['type of access being described', 'an aid pro...   \n",
       "10    ['+v_currency', '+v_glide', '+v_iso3', '+v_och...   \n",
       "11    ['ISO 4217 Currency Codes', 'GLobal IDEntifier...   \n",
       "12    ['Standard list of financial currency codes us...   \n",
       "13    ['International Organization for Standardizati...   \n",
       "14    ['https://en.wikipedia.org/wiki/ISO_4217', 'ht...   \n",
       "15    ['2376', '2377', '2379', '2380', '2381', '2382...   \n",
       "16    ['Administrative Officer', 'Head of Agency or ...   \n",
       "17    ['Responsible for Office Administration and ad...   \n",
       "18    ['Assistant administratif', \"Directeur d'agenc...   \n",
       "19    ['Oficial administrativo', 'Director de Agenci...   \n",
       "20    ['02/24/2017', '02/24/2017', '02/24/2017', '02...   \n",
       "21        [nan, nan, nan, nan, nan, nan, nan, nan, nan]   \n",
       "22        [nan, nan, nan, nan, nan, nan, nan, nan, nan]   \n",
       "23    ['Adamawa', 'Akwa Ibom', 'Anambra', 'Bauchi', ...   \n",
       "24    ['NG002', 'NG003', 'NG004', 'NG005', 'NG006', ...   \n",
       "25    ['Guinea', 'Guinea', 'Guinea', 'Guinea', 'Guin...   \n",
       "26     [nan, 'Open', nan, nan, nan, nan, nan, nan, nan]   \n",
       "27        ['Y', 'Y', 'Y', 'N', 'Y', 'N', 'Y', 'Y', 'Y']   \n",
       "28    ['01/01/2002', '09/10/2014', '01/01/2002', '01...   \n",
       "29    ['HM0096', 'MOH035', 'HM0097', 'HM0933', 'HM00...   \n",
       "...                                                 ...   \n",
       "5901                                              [nan]   \n",
       "5902                ['STP', 'STP', 'STP', 'STP', 'STP']   \n",
       "5903                        [nan, nan, nan, '202', nan]   \n",
       "5904  ['Not specified', 'Not specified', 'Not specif...   \n",
       "5905                  [nan, nan, nan, 'FXWAF0506', nan]   \n",
       "5906                 [nan, nan, nan, '2005-11-01', nan]   \n",
       "5907                 [nan, nan, nan, '2006-05-31', nan]   \n",
       "5908           ['2008', '2007', '2006', '2005', '2005']   \n",
       "5909                          [nan, nan, nan, nan, nan]   \n",
       "5910  ['591716', '1101695', '72044', '321004', '57136']   \n",
       "5911                          [nan, nan, nan, nan, nan]   \n",
       "5912  ['CHE', 'CHE', 'CHE', 'CHE', 'CHE', 'CHE', 'CH...   \n",
       "5913      [nan, nan, nan, nan, nan, nan, nan, nan, nan]   \n",
       "5914  ['Not specified', 'Not specified', 'Not specif...   \n",
       "5915      [nan, nan, nan, nan, nan, nan, nan, nan, nan]   \n",
       "5916      [nan, nan, nan, nan, nan, nan, nan, nan, nan]   \n",
       "5917      [nan, nan, nan, nan, nan, nan, nan, nan, nan]   \n",
       "5918  ['2021', '2020', '2019', '2018', '2017', '2016...   \n",
       "5919      [nan, nan, nan, nan, nan, nan, nan, nan, nan]   \n",
       "5920  ['406504', '2117307', '2939092', '2642377', '7...   \n",
       "5921      [nan, nan, nan, nan, nan, nan, nan, nan, nan]   \n",
       "5942  ['SDN', 'SDN', 'SDN', 'SDN', 'SDN', 'SDN', 'SD...   \n",
       "5943  ['635', '635', '635', '635', '635', '635', '63...   \n",
       "5944  ['Sudan 2018', 'Sudan 2018', 'Sudan 2018', 'Su...   \n",
       "5945  ['HSDN18', 'HSDN18', 'HSDN18', 'HSDN18', 'HSDN...   \n",
       "5946  ['2018-01-01', '2018-01-01', '2018-01-01', '20...   \n",
       "5947  ['2018-12-24', '2018-12-24', '2018-12-24', '20...   \n",
       "5948  ['2018', '2018', '2018', '2018', '2018', '2018...   \n",
       "5949  ['3980', '3981', '3982', '3983', '3985', '3986...   \n",
       "5950  ['Common Services', 'Education', 'Emergency Sh...   \n",
       "\n",
       "      Relative Column Position                                  Dataset_name  \\\n",
       "0                     0.100000               hxl-core-hashtag-schema.csv.CSV   \n",
       "1                     0.200000               hxl-core-hashtag-schema.csv.CSV   \n",
       "2                     0.300000               hxl-core-hashtag-schema.csv.CSV   \n",
       "3                     0.400000               hxl-core-hashtag-schema.csv.CSV   \n",
       "4                     0.500000               hxl-core-hashtag-schema.csv.CSV   \n",
       "5                     0.600000               hxl-core-hashtag-schema.csv.CSV   \n",
       "6                     0.700000               hxl-core-hashtag-schema.csv.CSV   \n",
       "7                     0.800000               hxl-core-hashtag-schema.csv.CSV   \n",
       "8                     0.900000               hxl-core-hashtag-schema.csv.CSV   \n",
       "9                     1.000000               hxl-core-hashtag-schema.csv.CSV   \n",
       "10                    0.200000                HXL master vocabulary list.CSV   \n",
       "11                    0.400000                HXL master vocabulary list.CSV   \n",
       "12                    0.600000                HXL master vocabulary list.CSV   \n",
       "13                    0.800000                HXL master vocabulary list.CSV   \n",
       "14                    1.000000                HXL master vocabulary list.CSV   \n",
       "15                    0.100000             Functional Roles (Beta) - CSV.CSV   \n",
       "16                    0.200000             Functional Roles (Beta) - CSV.CSV   \n",
       "17                    0.300000             Functional Roles (Beta) - CSV.CSV   \n",
       "18                    0.600000             Functional Roles (Beta) - CSV.CSV   \n",
       "19                    0.700000             Functional Roles (Beta) - CSV.CSV   \n",
       "20                    0.800000             Functional Roles (Beta) - CSV.CSV   \n",
       "21                    0.900000             Functional Roles (Beta) - CSV.CSV   \n",
       "22                    1.000000             Functional Roles (Beta) - CSV.CSV   \n",
       "23                    0.333333                     Nga_pop_adm1_2016.csv.CSV   \n",
       "24                    0.666667                     Nga_pop_adm1_2016.csv.CSV   \n",
       "25                    0.027778                  Guinea Health Facilities.CSV   \n",
       "26                    0.055556                  Guinea Health Facilities.CSV   \n",
       "27                    0.083333                  Guinea Health Facilities.CSV   \n",
       "28                    0.111111                  Guinea Health Facilities.CSV   \n",
       "29                    0.138889                  Guinea Health Facilities.CSV   \n",
       "...                        ...                                           ...   \n",
       "5901                  1.000000          fts_requirements_funding_twn.csv.CSV   \n",
       "5902                  0.100000          fts_requirements_funding_stp.csv.CSV   \n",
       "5903                  0.200000          fts_requirements_funding_stp.csv.CSV   \n",
       "5904                  0.300000          fts_requirements_funding_stp.csv.CSV   \n",
       "5905                  0.400000          fts_requirements_funding_stp.csv.CSV   \n",
       "5906                  0.500000          fts_requirements_funding_stp.csv.CSV   \n",
       "5907                  0.600000          fts_requirements_funding_stp.csv.CSV   \n",
       "5908                  0.700000          fts_requirements_funding_stp.csv.CSV   \n",
       "5909                  0.800000          fts_requirements_funding_stp.csv.CSV   \n",
       "5910                  0.900000          fts_requirements_funding_stp.csv.CSV   \n",
       "5911                  1.000000          fts_requirements_funding_stp.csv.CSV   \n",
       "5912                  0.100000          fts_requirements_funding_che.csv.CSV   \n",
       "5913                  0.200000          fts_requirements_funding_che.csv.CSV   \n",
       "5914                  0.300000          fts_requirements_funding_che.csv.CSV   \n",
       "5915                  0.400000          fts_requirements_funding_che.csv.CSV   \n",
       "5916                  0.500000          fts_requirements_funding_che.csv.CSV   \n",
       "5917                  0.600000          fts_requirements_funding_che.csv.CSV   \n",
       "5918                  0.700000          fts_requirements_funding_che.csv.CSV   \n",
       "5919                  0.800000          fts_requirements_funding_che.csv.CSV   \n",
       "5920                  0.900000          fts_requirements_funding_che.csv.CSV   \n",
       "5921                  1.000000          fts_requirements_funding_che.csv.CSV   \n",
       "5942                  0.083333  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "5943                  0.166667  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "5944                  0.250000  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "5945                  0.333333  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "5946                  0.416667  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "5947                  0.500000  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "5948                  0.583333  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "5949                  0.666667  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "5950                  0.750000  fts_requirements_funding_cluster_sdn.csv.CSV   \n",
       "\n",
       "                              Organization  Index  \n",
       "0     Humanitarian Exchange Language (HXL)      0  \n",
       "1     Humanitarian Exchange Language (HXL)      0  \n",
       "2     Humanitarian Exchange Language (HXL)      0  \n",
       "3     Humanitarian Exchange Language (HXL)      0  \n",
       "4     Humanitarian Exchange Language (HXL)      0  \n",
       "5     Humanitarian Exchange Language (HXL)      0  \n",
       "6     Humanitarian Exchange Language (HXL)      0  \n",
       "7     Humanitarian Exchange Language (HXL)      0  \n",
       "8     Humanitarian Exchange Language (HXL)      0  \n",
       "9     Humanitarian Exchange Language (HXL)      0  \n",
       "10    Humanitarian Exchange Language (HXL)      1  \n",
       "11    Humanitarian Exchange Language (HXL)      1  \n",
       "12    Humanitarian Exchange Language (HXL)      1  \n",
       "13    Humanitarian Exchange Language (HXL)      1  \n",
       "14    Humanitarian Exchange Language (HXL)      1  \n",
       "15                   OCHA Digital Services      2  \n",
       "16                   OCHA Digital Services      2  \n",
       "17                   OCHA Digital Services      2  \n",
       "18                   OCHA Digital Services      2  \n",
       "19                   OCHA Digital Services      2  \n",
       "20                   OCHA Digital Services      2  \n",
       "21                   OCHA Digital Services      2  \n",
       "22                   OCHA Digital Services      2  \n",
       "23                            OCHA Nigeria      3  \n",
       "24                            OCHA Nigeria      3  \n",
       "25                      Standby Task Force      4  \n",
       "26                      Standby Task Force      4  \n",
       "27                      Standby Task Force      4  \n",
       "28                      Standby Task Force      4  \n",
       "29                      Standby Task Force      4  \n",
       "...                                    ...    ...  \n",
       "5901                              OCHA FTS    336  \n",
       "5902                              OCHA FTS    337  \n",
       "5903                              OCHA FTS    337  \n",
       "5904                              OCHA FTS    337  \n",
       "5905                              OCHA FTS    337  \n",
       "5906                              OCHA FTS    337  \n",
       "5907                              OCHA FTS    337  \n",
       "5908                              OCHA FTS    337  \n",
       "5909                              OCHA FTS    337  \n",
       "5910                              OCHA FTS    337  \n",
       "5911                              OCHA FTS    337  \n",
       "5912                              OCHA FTS    338  \n",
       "5913                              OCHA FTS    338  \n",
       "5914                              OCHA FTS    338  \n",
       "5915                              OCHA FTS    338  \n",
       "5916                              OCHA FTS    338  \n",
       "5917                              OCHA FTS    338  \n",
       "5918                              OCHA FTS    338  \n",
       "5919                              OCHA FTS    338  \n",
       "5920                              OCHA FTS    338  \n",
       "5921                              OCHA FTS    338  \n",
       "5942                              OCHA FTS    341  \n",
       "5943                              OCHA FTS    341  \n",
       "5944                              OCHA FTS    341  \n",
       "5945                              OCHA FTS    341  \n",
       "5946                              OCHA FTS    341  \n",
       "5947                              OCHA FTS    341  \n",
       "5948                              OCHA FTS    341  \n",
       "5949                              OCHA FTS    341  \n",
       "5950                              OCHA FTS    341  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "headers_and_tags.head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers_and_tags.to_excel('processed_input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_randomly(dataset, threshold = 200):\n",
    "    #ensures that dataset isn't skewed towards particular tags by ensuring that each tag has at most a given number \n",
    "    #(default: 200) of rows defined by the threshold\n",
    "    new_dataset = dataset\n",
    "    tags_to_be_pruned = dataset['Tag'].value_counts()[dataset['Tag'].value_counts() > threshold].keys().tolist()\n",
    "    for tag in tags_to_be_pruned:\n",
    "        count = dataset['Tag'][dataset['Tag'] == tag].value_counts()\n",
    "        drop_count = count - threshold\n",
    "        drop_count = drop_count.tolist()[0]\n",
    "        new_dataset = new_dataset.drop(np.random.choice(new_dataset[new_dataset['Tag']==tag].index,size=drop_count,replace=False))\n",
    "    return new_dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample the data to avoid favoring frequently appearing words\n",
    "if (use_randomized_sample_for_training_data):\n",
    "    headers_and_tags = select_randomly(headers_and_tags)\n",
    "    headers_and_tags.to_excel('processed_input_random_selection.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implementing n-grams Model (not used in this model.)\n",
    "from nltk import ngrams\n",
    "\n",
    "def generate_n_grams(data_lst, n):\n",
    "    cleaned = remove_chars(list(data_lst))\n",
    "    cleaned = clean_cols(cleaned)\n",
    "    cleaned = remove_stop_words(cleaned) \n",
    "    return list(ngrams(cleaned, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a n-gram frequency table \n",
    "\n",
    "def count_stats_grams(two_d_arr):\n",
    "    lst = np.array([])\n",
    "    count = 0\n",
    "    singles_count = 0\n",
    "    multiples_count = 0\n",
    "    for arr in two_d_arr:\n",
    "        if arr not in lst:\n",
    "            count += 1\n",
    "            np.append(lst, arr)\n",
    "        if two_d_arr.count(arr) == 1:\n",
    "            singles_count += 1\n",
    "        if two_d_arr.count(arr) > 1:\n",
    "            multiples_count += 1\n",
    "    check = count - singles_count\n",
    "    assert(check == multiples_count)\n",
    "    return count, singles_count, multiples_count\n",
    "\n",
    "def n_gram_freqs(dataframe, max_n = 4):\n",
    "    n_gram_cols = ['n-gram', 'data' ,'unique ngrams', 'multiples', 'singles']\n",
    "    n_gram_freqs = pd.DataFrame(columns = n_gram_cols)\n",
    "    for i in range(max_n):\n",
    "        n = i+1\n",
    "        n_grams = generate_n_grams(dataframe['Header'], n)\n",
    "        unique_n_grams, singles, multiples = count_stats_grams(n_grams)\n",
    "        row = {'n-gram': n, \n",
    "              'data': n_grams,\n",
    "              'unique ngrams': unique_n_grams,\n",
    "              'multiples': multiples,\n",
    "              'singles': singles}\n",
    "        n_gram_freqs.loc[len(n_gram_freqs)] = row\n",
    "    return pd.DataFrame(n_gram_freqs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes a data row and cleans it for model input\n",
    "def word_extract(row):\n",
    "    ignore = ['nan']\n",
    "    no_white = [i.lstrip() for i in row if i not in ignore and not isinstance(i, float)]\n",
    "    cleaned_text = [w.lower() for w in no_white if w not in ignore]\n",
    "    return cleaned_text\n",
    "\n",
    "long_string = []\n",
    "for i in headers_and_tags['Data']:\n",
    "    result_by_tag = word_extract(i)\n",
    "    holder_list = ''.join(result_by_tag)\n",
    "    long_string.append(holder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loading in the pre-determined word vectors\n",
    "fasttext_model = 'wiki.en.bin'\n",
    "fmodel = load_model(fasttext_model)\n",
    "print(\"Pre-trained model loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper function to extract the top n most likely tags for a given column (3 top tags selected in default.)\n",
    "#returns a dictionary where key = header and values = dictionary of num_of_top_tags tags with their respective probabilities\n",
    "\n",
    "def top_tags(clf, X_test, series, num_of_top_tags = 3):\n",
    "    if (not isinstance(X_test, np.ndarray)):\n",
    "        X_test = X_test.values.tolist()\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    values = []\n",
    "    for i in range(len(X_test)):\n",
    "        max_args = probs[i].argsort()[-num_of_top_tags:][::-1]\n",
    "        top_suggested_tags = clf.classes_[max_args]\n",
    "        dictionary = {}\n",
    "        sorted_probs = np.take(probs[i], max_args)\n",
    "        key = series.iloc[i]\n",
    "        for j in range(len(top_suggested_tags)):\n",
    "            dictionary[top_suggested_tags[j]]=sorted_probs[j]\n",
    "        values.append((key, dictionary))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#helper functions to determine the confidence level of top predicted tag. Any tags with a confidence level < 0.5\n",
    "#will be discarded and replaced by a blank tag instead. The function returns an array of booleans determining whether \n",
    "#a given tag will be blank or not. \n",
    "\n",
    "def tag_predicted(clf, X_test, series, threshold):\n",
    "    #True if tag should be left blank\n",
    "    if (not isinstance(X_test, np.ndarray)):\n",
    "        X_test = X_test.values.tolist()\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    values = []\n",
    "    for i in range(len(X_test)):\n",
    "        max_arg = probs[i].argsort()[-1]\n",
    "        top_suggested_tag = clf.classes_[max_arg]\n",
    "        prob = np.take(probs[i], max_arg)\n",
    "        if (prob > threshold):\n",
    "            values.append(False)\n",
    "        else:\n",
    "            values.append(True)\n",
    "    return values\n",
    "\n",
    "#helper function to fill in the blanks for tags that have a confidence level less than the threshold\n",
    "def fill_blank_tags(predicted_tags, clf, X_test, series, threshold = 0.5):\n",
    "    boolean_array = tag_predicted(clf, X_test, series, threshold)\n",
    "    for i in range(len(predicted_tags)):\n",
    "        if (boolean_array[i] == True):\n",
    "            predicted_tags[i] = ''\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#helper function to extract n random data points from the data column and vectorizes them into columns\n",
    "def separate_words(series): \n",
    "    #each series is a long string that contains all the data\n",
    "    if (not isinstance(series, str)):\n",
    "        series = str(series)\n",
    "    lst = re.split(r\"[^a-zA-Z0-9_.]\", series)\n",
    "    lst = list(filter(None, lst))\n",
    "    return lst\n",
    "    \n",
    "def vectorize_n_datapoints(number_of_datapoints_to_vectorize = 7):\n",
    "    df['Data_separated'] = df['Data'].apply(separate_words)\n",
    "    if (number_of_datapoints_to_vectorize > len(df['Data_separated'][0])):\n",
    "        number_of_datapoints_to_vectorize = len(df['Data_separated'][0])\n",
    "    for i in range(number_of_datapoints_to_vectorize):\n",
    "        df['datapoint' + str(i)] = df['Data_separated'].str[i]\n",
    "        \n",
    "def embedded_datapoints(number_of_data_point_to_vectorize = 7):\n",
    "    vectorize_n_datapoints()\n",
    "    for i in range(number_of_data_point_to_vectorize):\n",
    "        df['embedded_datapoint' + str(i)] = df['datapoint' + str(i)].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "        \n",
    "number_of_data_point_to_vectorize = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#post-processing function that 1) checks tags with low confidence against mappings 2) fills in a blank prediction for \n",
    "#tags with a confidence level lower than threshold and had no obvious mappings associated with the predicted tag. \n",
    "#The function returns 1) the count of corrected tags 2) predicted and predicted tags\n",
    "\n",
    "def check_mapping(header, predicted_tag):\n",
    "    MAPPINGS = {\n",
    "    \"#geo\" : ['lon', 'lat', 'latitude', 'longitude'], #words that would likely appear for #geo tag\n",
    "    \"#admin\" : ['county'], #words that would likely appear for #admin tag\n",
    "    \"#country\" :  ['country'], #words that would likely appear for #country tag\n",
    "    \"#date\" : ['year', 'date'], #words that would likely appear for #date tag\n",
    "    \"#funding\": ['funding', 'funded'], #words that would likely appear for #funding tag\n",
    "    \"#value\": ['percentfunded'], #words that would likely appear for #value tag\n",
    "    \"#org\":['organization', 'funder ref', 'org'], #words that would likely appear for #org tag\n",
    "    \"#status\":['status'], #words that would likely appear for #status tag\n",
    "    \"#sector\":['sector'], #words that would likely appear for #sector tag\n",
    "    \"#adm1\":['adm1', 'admin1'], #words that would likely appear for #adm1 tag\n",
    "    \"#adm2\":['adm2', 'admin2'], #words that would likely appear for #adm2 tag\n",
    "    \"#adm3\":['adm3', 'admin3'], #words that would likely appear for #adm3 tag\n",
    "    \"#adm4\":['adm4', 'admin4']  #words that would likely appear for #adm4 tag        \n",
    "    }\n",
    "    change_tag = False\n",
    "    header_words = header.split()\n",
    "    for key, val in MAPPINGS.items():\n",
    "        for word in header_words:\n",
    "            #check if the header contains any of the words in the mappings (substrings are not included)\n",
    "            if (word in val):\n",
    "                if (predicted_tag != key):\n",
    "                    predicted_tag = key\n",
    "                    change_tag = True\n",
    "    return change_tag, predicted_tag\n",
    "    \n",
    "\n",
    "def post_processing(headers, predicted_tags, clf, X_test, mapping_threshold = 0.85, blank_threshold = 0.2):\n",
    "    if (not isinstance(X_test, np.ndarray)):\n",
    "        X_test = X_test.values.tolist()\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    values = []\n",
    "    corrected_count = 0\n",
    "    blank_count = 0\n",
    "    for i in range(len(X_test)):\n",
    "        max_arg = probs[i].argsort()[-1]\n",
    "        top_suggested_tag = clf.classes_[max_arg]\n",
    "        prob = np.take(probs[i], max_arg)\n",
    "        header = headers.tolist()[i]\n",
    "        predicted_tag = predicted_tags[i]\n",
    "        if (prob < mapping_threshold):\n",
    "            inc, predicted_tag = check_mapping(header, predicted_tag)\n",
    "            if (inc):\n",
    "                corrected_count += 1\n",
    "            else:\n",
    "                if (prob < blank_threshold): \n",
    "                    predicted_tag = ''\n",
    "                    blank_count += 1\n",
    "        values.append(predicted_tag)\n",
    "    return corrected_count, blank_count, values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper function to add hashtags to the predicted tags\n",
    "def add_hashtags(predicted_tags):\n",
    "    if (isinstance(predicted_tags, np.ndarray)):\n",
    "        return [\"#\"+word for word in predicted_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings extracted!\n",
      "\n",
      "Features created!\n"
     ]
    }
   ],
   "source": [
    "#CREATING THE MODEL and TESTING ACCURACY OF MLP Classifer on all features. This is the model used in the API.\n",
    "\n",
    "#using all three embedded features to predict tags\n",
    "#cleaning the dataset by flattening the datastructure\n",
    "\n",
    "df = headers_and_tags\n",
    "\n",
    "df['Header_embedding'] = df['Header'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "df['Organization_embedded'] = df['Organization'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "embedded_datapoints()\n",
    "print(\"Word embeddings extracted!\\n\")\n",
    "\n",
    "df['data_combined'] = df.loc[:, 'embedded_datapoint0': 'embedded_datapoint' \n",
    "                                                           + str(number_of_data_point_to_vectorize-1)].values.tolist()\n",
    "df['data_combined'] = df['data_combined'].apply(lambda x: [val for item in x for val in item])\n",
    "\n",
    "df_target = df\n",
    "cols = ['Header_embedding', 'Organization_embedded', 'data_combined']\n",
    "df_target['features_combined'] = df_target[cols].values.tolist()\n",
    "df_target['features_combined'] = df_target['features_combined'].apply(lambda x: [val for item in x for val in item])\n",
    "print(\"Features created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags corrected: 18\n",
      "Number of blank tags: 35\n",
      "Classification accuracy on test set: 0.943056943057\n"
     ]
    }
   ],
   "source": [
    "#Outputs: predicted_tags = 1D array including the most likely tags for each column\n",
    "#         top_3_predicted_tags = a dictionary of 3 most likely tags with their respective probabilities for a given column\n",
    "#         pickle file = stores the classifier to be inputted into the API\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_target['features_combined'], \n",
    "                                                    df['Tag'][0:len(df_target['features_combined'])], \n",
    "                                                    test_size=0.33, random_state=0)\n",
    "\n",
    "_1, headers, _2, _3 = train_test_split(df_target['Header'][0:len(df_target['features_combined'])], \n",
    "                                       df['Tag'][0:len(df_target['features_combined'])], test_size = 0.33, random_state = 0)\n",
    "\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "clf.fit(list(X_train), y_train)\n",
    "predicted_tags = clf.predict(list(X_test))\n",
    "predicted_tags = add_hashtags(predicted_tags)\n",
    "\n",
    "corrected_count, blank_count, predicted_tags = post_processing(headers, predicted_tags, clf, X_test)\n",
    "print(\"Number of tags corrected: \" + str(corrected_count))\n",
    "print(\"Number of blank tags: \" + str(blank_count))\n",
    "\n",
    "top_3_predicted_tags = top_tags(clf, X_test, df['Header'])\n",
    "#predicted_tags = fill_blank_tags(predicted_tags, clf, X_test, df['Header'])\n",
    "test_score = clf.score(list(X_test), y_test)\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)\n",
    "pickle.dump(clf,open(\"model.pkl\",\"wb\"))\n",
    "pickle.dump(predicted_tags, open(\"predicted_tags.pkl\",\"wb\"))\n",
    "pickle.dump(top_3_predicted_tags, open(\"top_three_predicted_tags.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CELLS BELOW ARE MEANT FOR TESTING PURPOSES. THEY SERVE NO PURPOSE FOR THE MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count, predicted_tags = post_processing(headers, predicted_tags, clf, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags corrected: 13\n"
     ]
    }
   ],
   "source": [
    "count, predicted_tags = check_mappings(headers, predicted_tags)\n",
    "print(\"Number of tags corrected: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10973            disp date\n",
       "10974       orig prov code\n",
       "10975       orig prov name\n",
       "10976       orig dist code\n",
       "10977       orig dist name\n",
       "10978       disp prov code\n",
       "10979       disp prov name\n",
       "10980       disp dist code\n",
       "10981       disp dist name\n",
       "10982             disp ind\n",
       "10983             disp fam\n",
       "10984      disp adult male\n",
       "10985    disp adult female\n",
       "10986    disp children u18\n",
       "Name: Header, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 5000)\n",
    "df_target['Header'][1056:1070]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['date', 'adm2', 'adm2', 'adm3', 'adm3', 'adm2', 'adm2', 'adm3',\n",
       "       'adm3', 'affected', 'affected', 'affected', 'affected', 'affected'],\n",
       "      dtype='<U22')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_train, x_test_test, y_test_train, y_test_test = train_test_split(\n",
    "df_target['Header_embedding'][1500:], df['Tag'][1500:], test_size = 0, random_state = 0)\n",
    "clf2 = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "clf2.fit(list(x_test_train), y_test_train)\n",
    "#df_target['Header'][1056:1070]\n",
    "test_arr = pd.DataFrame(['PooledFundName', 'ExternalProjectCode', 'ProjectStatus', 'ProcessStatus'])\n",
    "test = test_arr.map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "test_tags = clf2.predict(list(test))\n",
    "test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on test set: 0.943722943723\n"
     ]
    }
   ],
   "source": [
    "top_3_predicted_tags = top_tags(clf, X_test, df['Header'])\n",
    "predicted_tags = fill_blank_tags(predicted_tags, clf, X_test, df['Header'])\n",
    "test_score = clf.score(list(X_test), y_test)\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)\n",
    "pickle.dump(clf,open(\"model.pkl\",\"wb\"))\n",
    "pickle.dump(predicted_tags, open(\"predicted_tags.pkl\",\"wb\"))\n",
    "pickle.dump(top_3_predicted_tags, open(\"top_three_predicted_tags.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors\n",
      "0.904095904096\n",
      "Random Forest\n",
      "0.67698967699\n",
      "Neural Net\n",
      "0.94338994339\n",
      "Naive Bayes\n",
      "0.625041625042\n"
     ]
    }
   ],
   "source": [
    "#Testing The Accuracy for Other Models using all three features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "names = [\"Nearest Neighbors\",\n",
    "         \"Random Forest\", \"Neural Net\",\n",
    "         \"Naive Bayes\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    #SVC(kernel=\"linear\", C=0.025),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),#use graph search? toggle max_features\n",
    "    MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam'),\n",
    "    GaussianNB()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "i = 1\n",
    "for name, clf in zip(names, classifiers):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_target['features_combined'], \n",
    "                                                    df['Tag'][0:len(df_target['features_combined'])], \n",
    "                                                    test_size=0.33, random_state=0) \n",
    "        clf.fit(list(X_train), y_train)\n",
    "        score = clf.score(list(X_test), y_test)\n",
    "        print(name)\n",
    "        print(score)\n",
    "        \n",
    "\n",
    "#Accuracy of MLP: 95.10%\n",
    "\n",
    "#Models Considered\n",
    "#1) Gaussian Naive Bayes       \n",
    "    #Accuracy: 88.03%\n",
    "#2) RandomForest       \n",
    "    #Accuracy: 98.36%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
